# ML-for-zju

![image-20231022102907303](C:\Users\23828\AppData\Roaming\Typora\typora-user-images\image-20231022102907303.png)

generative model 生成式模型和非生成式模型

linear model/none linear model

![image-20231022103347613](C:\Users\23828\AppData\Roaming\Typora\typora-user-images\image-20231022103347613.png)

生成模型（Generative Model）和判别模型（Discriminative Model）是机器学习中两种不同类型的模型，它们的主要区别在于它们对数据的建模方式和应用领域。

1. **生成模型（Generative Model）**：
   - 生成模型试图对整个数据分布进行建模，它学习如何生成数据。生成模型能够生成与训练数据集相似的新数据点。
   - 生成模型通常用于生成新数据、样本合成、缺失数据填补以及密度估计等任务。
   - 代表性的生成模型包括变分自编码器（Variational Autoencoders，VAE）、生成对抗网络（Generative Adversarial Networks，GAN）、隐马尔可夫模型（Hidden Markov Models，HMM）等。
   - 生成模型通常需要更多的数据和计算资源来训练，因为它们要学习整个数据分布。
2. **判别模型（Discriminative Model）**：
   - 判别模型关注的是对不同类别之间的边界进行建模，它学习如何对输入数据进行分类或标记。
   - 判别模型通常用于分类、回归和标记任务，其中目标是预测输入数据的标签或属性。
   - 代表性的判别模型包括逻辑回归、支持向量机、决策树、神经网络中的前馈神经网络等。
   - 判别模型通常在小规模数据上效果较好，因为它们只需要建模与任务相关的部分信息，而不需要学习整个数据分布。

总结来说，生成模型和判别模型的关键区别在于其建模目标。生成模型试图理解整个数据分布并生成新数据，而判别模型关注的是对数据的分类或标记。在实际应用中，选择生成模型还是判别模型通常取决于具体问题和任务的性质。有时，这两种类型的模型可以结合使用，以实现更复杂的机器学习任务。



![image-20231022110143358](D:\notes\assets\image-20231022110143358.png)

linear feature+nonlinear model/ nonlinear feature+linear model是什么意思



ChatGPT



"线性特征 + 非线性模型" 和 "非线性特征 + 线性模型" 是机器学习中的两种不同方法，用于处理数据和建立预测模型。它们可以在特征工程和模型选择方面用于处理非线性关系。

1. **线性特征 + 非线性模型**：
   - 在这种方法中，特征是原始数据或线性组合的结果，而模型是非线性的。这意味着模型可以学习非线性关系，但特征本身是线性的。
   - 举例：特征可以是原始数据，如长度、宽度和高度，但模型可以是多项式回归模型，能够拟合非线性关系，例如长度和宽度的乘积对输出的影响。
2. **非线性特征 + 线性模型**：
   - 在这种方法中，特征本身是经过非线性变换的，例如，通过应用某种非线性函数（例如对数、指数、多项式等）对原始数据进行处理。然后，线性模型被用于学习特征之间的线性关系。
   - 举例：特征可以是原始数据的对数，模型可以是线性回归模型，通过对数变换能够捕捉数据中的非线性关系，如指数增长。

这两种方法通常用于处理不同类型的数据和问题。选择哪种方法通常取决于问题的性质以及数据的分布。在某些情况下，可以尝试两种方法，然后根据模型性能来选择最佳方法。

需要注意的是，"非线性特征 + 线性模型" 也可以被看作是一种"线性特征 + 非线性模型"的特殊情况，因为线性模型本身可以学习非线性关系，只要特征经过适当的非线性变换。这种灵活性使得线性模型成为许多机器学习问题的有力工具。



| Truth/Prediction | Positive |      |
| :--------------- | -------- | ---- |
| Positive         | TP       |      |
| Negative         | FP       |      |
|                  |          |      |

![image-20231022110708240](D:\notes\assets\image-20231022110708240.png)

mean-squared error

![image-20231022112204191](D:\notes\assets\image-20231022112204191.png)

![image-20231022113633010](D:\notes\assets\image-20231022113633010.png)

# 10-25

### symbolism

logic theorist

expert system

automated theoroem proving



### connectionism

transformers

recurrent neural networks

convolution neural netorks

# Neyman-Pearson方法

Neyman-Pearson方法是统计学中的一种假设检验方法，由Jerzy Neyman和Egon Pearson命名。它在频率主义统计学中被广泛应用，用于根据观察到的数据在两个竞争性假设之间做出决策。该方法着重于控制第一类错误的概率，即当真实的零假设被错误拒绝时发生的错误。

以下是Neyman-Pearson方法的关键要点：

1. **零假设（$H_0$）和备择假设（$H_1$或$H_a$）：**
   - $H_0$：零假设代表默认假设，通常是无效果或无差异的陈述。
   - $H_1$：备择假设表示与零假设相对的陈述，通常涉及效果或差异的存在。
2. **显著性水平（Significance Level）：**
   - 定义：显著性水平是在**零假设为真的情况下拒绝零假设的概率**。
   - 应用：研究者在实验设计时选择显著性水平，通常用符号 �*α* 表示。
3. **试验统计量（Test Statistic）：**
   - 定义：一个数值，根据观察到的数据计算而得，用于决定是否拒绝零假设。
   - 应用：根据试验统计量与临界值（由显著性水平确定）的关系来做出决策。
4. **拒绝域和接受域（Rejection Region and Acceptance Region）：**
   - 定义：拒绝域是试验统计量的值，当它落在这个区域内时，拒绝零假设。接受域则是拒绝域的补集。
   - 应用：通过选择拒绝域的边界，研究者可以控制第一类错误的概率。

Neyman-Pearson方法通过在控制显著性水平的同时最小化第二类错误的概率，提供了一种强假设检验的框架。这种方法在实际应用中对于需要明确控制错误率的问题非常有用。

## posterior = beta.pdf(p, a, b)是什么意思

这个表达式表示了后验概率的计算方式，其中**``beta.pdf(p, a, b)``**表示了一个Beta分布的概率密度函数（probability density function，PDF）。在这个表达式中，**``p``**是概率的取值，**``a``**和**``b``**是Beta分布的参数。

Beta分布是一个常用的概率分布，它在概率论和统计学中经常被用于描述随机变量的取值范围在[0, 1]之间的情况。它的概率密度函数可以表示为：

$$ \text{Beta}(p|a, b) = \frac{1}{B(a, b)} \cdot p^{a-1} \cdot (1-p)^{b-1} $$ 

其中，$$ \text{Beta}(p|a, b)$$是Beta函数（Beta function），用于归一化Beta分布。在后验概率的计算中，通常会假设先验分布为Beta分布，并结合观测数据，通过贝叶斯定理计算得到后验分布。这里的**``beta.pdf(p, a, b)``**就是计算给定参数**``a``**和**``b``**的Beta分布在概率值**``p``**处的概率密度。

因此，**``posterior = beta.pdf(p, a, b)``**表示计算后验概率，即给定先验分布和观测数据，通过Beta分布的概率密度函数计算在概率值**``p``**处的后验概率密度。

# Metropolis-Hastings算法

Metropolis-Hastings算法是一种用于从目标分布中抽样的马尔可夫链蒙特卡罗（MCMC）方法。这个算法允许我们在无法直接从目标分布中抽样的情况下，通过构建一个马尔可夫链来间接地生成样本。以下是一个通俗的解释：

1. **背景：**
   - **目标分布（Target Distribution）：** 我们想要从中抽样的分布，通常是由于它很难直接从中抽样。
   - **马尔可夫链（Markov Chain）：** 一系列随机变量的序列，其中每个变量的状态仅依赖于前一个状态。
2. **思想：**
   - 我们构建一个马尔可夫链，使其平稳分布（稳态分布）为我们想要抽样的目标分布。
3. **步骤：**
   - **提议步骤（Proposal Step）：** 从当前状态生成一个提议状态。这可以通过从某个简单分布中抽样来实现。
   - **接受/拒绝步骤（Accept/Reject Step）：** 以一定的概率接受提议状态，否则保持当前状态。这个概率由目标分布和提议分布的比例决定。
4. **具体流程：**
   - 从当前状态开始，通过提议步骤生成一个新的状态。
   - 计算接受概率，它考虑了目标分布在新状态和当前状态下的概率密度比。
   - 根据接受概率决定是否接受新状态。如果接受，则更新为新状态；否则，保持当前状态。
   - 重复这个过程，得到一个马尔可夫链。
5. **收敛性：**
   - 随着抽样次数的增加，马尔可夫链趋向于稳态分布，从而生成的样本趋近于目标分布。

总体而言，Metropolis-Hastings算法通过引入提议步骤和接受/拒绝步骤，利用马尔可夫链的性质，逐步探索并逼近目标分布，从而实现从目标分布中抽样的目的。这种方法在贝叶斯统计学、统计物理学等领域中广泛应用。